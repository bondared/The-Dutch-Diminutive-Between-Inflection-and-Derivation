\chapter{Study}\label{chp:study}
The previous chapter saw us adopt the diminutive suffix analysis proposal advanced by \citeauthor{DeBelder+etal+2014} (\citeyear{DeBelder+etal+2014}) and establish predictions for empirical testing of this proposal in a lexical decision experiment. This chapter describes the practical implementation of the experiment and delves into the methodological and theoretical considerations that went into the decision-making process regarding experimental desig, practical implementation of the design, and subsequent analysis.

\section{Design}
The usual way to test any predictions regarding word recognition is to collect lexical decision data from native speakers of the language(s) in question. However, the process of data collection can be cost-intensive and time-consuming: speakers need to be recruited and vetted based on the aims of the experiment, which is a lengthy process at the best of times. Additionally, participants are usually compensated for their time with money, and studies with large groups of people can quickly become very expensive. On the other hand, experiments involving very small groups of people have little to no explanatory value, as the population sample is simply too small for it to allow accurate predictions about the population at large. Low participant numbers can thus lead to statistically underpowered studies, making the analysis inconclusive at best (\citeauthor{Winter+2019}, \citeyear{Winter+2019}). Software and equipment costs should also be taken into account. The researcher, then, has no choice but to commit significant resources to novel data collection for their experiment, which usually makes it economically unviable for undergraduate students looking to test novel predictions. However, previously established lexical decision corpora such as the Dutch Lexicon Project (DLP2; \cite{Brysbaert+etal+2016}) can provide a convenient alternative.

Corpora like that typically serve multiple goals: as usual, data collection starts by looking to test a novel prediction; however, additional information is also collected or imported from other similar corpora. After the experiment is concluded and possibly reported, the resulting corpus is made publicly available to serve as basis for virtual experiments. In fact, some previously established corpora are later used by their creators as auxiliary datasets to test new predictions and compare effects. For example, while testing \citeauthor{Brysbaert+etal+2016} (\citeyear{Brysbaert+etal+2016}) have made reference to the first Dutch Lexicon Project

The DLP2 corpus was chosen as the 

\begin{itemize}
\item Introduce lexical decision data corpora and the idea of a simulated experiment (cite \cite{Brysbaert+etal+2016})
\item Outline the corpus, describe the advantages, mention the limitations \par

\end{itemize}
\section{Implementation}
The entire process of data preparation and analysis was carried out in RStudio (\citeauthor{RStudio}, \citeyear{RStudio}), an open-access development environment for the statistical programming language R (\citeauthor{rlang}, \citeyear{rlang}). All supplementary materials including the experimental dataset, preprocessing and analysis scripts, and digital figures can be found in this OSF repository: \url{https://osf.io/zxjgw/}. 

Data import and preparation was carried out using R packages \texttt{dplyr} (\citeauthor{dplyr+2022} \citeyear{dplyr+2022}) and \texttt{readr} (\citeauthor{readr+2022}, \citeyear{readr+2022}). The implementation of the virtual experiment proceeded as follows: first, all constituent datasets of the DLP2 corpus were downloaded from \href{http://crr.ugent.be/archives/1796}{its webpage} and imported into Rstudio. The \texttt{items} and \texttt{variables} subcorpora were then merged in order to combine values for lexicality and experimental predictors such as frequency and length in one dataset. Subsequent exclusion of all nonwords from the resulting dataset and filtering it to only include nouns allowed for easier access to the nominal diminutive forms. This subset was then searched for wordforms ending in \textit{-je} to include all possible allomoprhs of the diminutive suffix, resulting in a list of 364 words.

The novel diminutive predictor, named \texttt{dim\_type}, was then set up within the list of diminutives, respectively coding derivational and inflectional forms as \texttt{deriv} and \texttt{infl}. The values for \texttt{dim\_type} were manually assigned, based on the  distinguishing features of the two diminutive types: an item was deemed inflectional if its meaning was compositional (usually reflecting the "small X" pattern) and/or if the process of suffixation had forced a kind-to-item reading shift. Conversely, items with meaning gaps (lexicalised diminutives and \textit{diminutiva tantum}) were counted as derivational. Irregular patterns of allomorphy served as an additional indicator of an item belonging to the derivational category, e.g. \textit{bloem-pje} "flower+DIM" features the predicted allomorph -\textit{pje} and is treated as inflected, \textit{bloem-etje} has a different allomorph -\textit{pje} and additionally means "bouquet" instead, hence its treatment as derived. 

All assigned values were double-checked with the help of native speaker intuitions, as well as additional consultation of online Dutch dictionaries such as the digital version of Van Dale (\url{https://www.vandale.nl/}) and the Dutch version of Wiktionary (\url{https://www.nl.wiktionary.org}), when necessary. Items that had at least one of the inflectional and one of the derivational type readings at the same time were assigned the value \texttt{both} and slated for exclusion from the analysis due to their unpredictability.

An additional variable coding each word for the number of constituent morphemes (\texttt{nmorph}) was set up within the list of diminutives due to the DLP2 dataset not featuring one in the first place. Due to theoretical assumptions of full decomposition in visual word processing, formal diminutives with no independently occurring base and/or idiosyncratic patterns of meaning were still treated as compositional. Conversely, items ending in \textit{-je} that upon inspection were simplex forms such as \textit{flottielje} "flotilla" and \textit{kastanje} "chestnut" were excluded from the list. Thus, every item on the list was confirmed to be a diminutive and assigned a value corresponding to its morphological type, and none were assigned an \texttt{nmorph} value lower than 2. The resulting list constituted 356 items in total.

Following the assignment procedure, the diminutive list was then merged with the noun dataset to reintroduce the values for experimental predictors, making up the subcorpus of purely diminutive items. This was subsequently combined with the subcorpora containing raw trial data from the participants recruited for the DLP2 project. An additional cleanup stage filtered out all observations where participants failed to recognise a diminutive as a word, as well as observations for all diminutive items with a mean accuracy lower than 66 per cent. Trials that took longer than 1250 ms were excluded as well. This was intended as an intermediary solution between using 1000ms as the threshold (\cite{Brysbaert+etal+2016} would tell their participants to speed up for the next block if they took longer than 1000ms on average; participants who got three such reminders had all of their responses wiped from the final version of the DLP2) and letting the more extreme values, some going beyond 1500ms, induce an undesirable positive skew in the model used for analysis. Finally, observations for items coded \texttt{both} for \texttt{dim\_type} were excluded from the analysis, resulting in an experimental dataset of 9785 observations across a total of 270 diminutive items.

\section{Analysis}

The formula for the experimental model was intended to closely follow the one used for reaction times in \citeauthor{Brysbaert+etal+2016} (\citeyear{Brysbaert+etal+2016}); however, due to the differences between the diminutive subset and the entirety of the DLP2, as well as the introduction of \texttt{nmorph} as a predictor, some modifications had to be made. A series of correlation tests between length, number of syllables, \texttt{nmorph}, and orthographic similarity effects revealed a high degree of intercorrelation (average Pearson's $r=0.7937$). Estimating the effects of all four predictors as part of one model would lead to unreliable predictions at best, and so some of them had to be excluded. Due to \texttt{nmorph} being theory-relevant, it was decided to keep it to the detriment of some other predictors. Number of syllables, being the least relevant predictor, was excluded first. Next, length was excluded in favour of orthographic similarity scores, after a series of stepwise model comparisons confirmed that the latter contributed more to explaining the variance in the dataset.

 The observations in the dataset were assumed not to be fully independent for two key reasons.  Firstly, in the design of the DLP2 data collection process, items were repeated across participants. Secondly, the values of \texttt{dim\_type} vary within individuals, as the predictor was not part of the initial design. Thie data analysis step therefore required using mixed-effects linear regression models to account for possible interdependencies. In order to satisfy the independence assumption for each data point, random intercepts by participant and by item were introduced into the model formula (\cite{Winter+2019}). Introducing an additional random slope by participant resulted in a model with a singular fit, and so the participant slope was removed. To test for possible interaction effects between frequency and diminutive type (\textbf{CITE SOMETHING AS PRECEDENT}), an interaction term was introduced as well; a quick comparison using the \texttt{anova()} function confirmed the interaction model to have a better fit to the data. The formula for the complete experimental model is presented in (\ref{ex:modelformula}) in Section \ref{sec:inf_stats}, followed by a discussion of all fixed and random effects.

Reaction times were log-transformed to alleviate the typical positive skew towards longer RTs, as suggested in \citeauthor{Winter+2019} (\citeyear{Winter+2019}). In order to make the difference in-between their effect sizes more comparable, all the continuous predictors were standardised and centered. This process involved subtracting each variable's mean from every data point and then dividing each value of a centered variable by its standard deviation. The values of every continuous predictor were thus linearly transformed into standard units, or \textit{z}-scores, representing how far they were from their respective predictor's mean in terms of standard deviations (\citeauthor{Winter+2019}, \citeyear{Winter+2019}). The only categorical predictor, diminutive type, was sum-coded. \texttt{infl} was assigned a contrast value of -1, while \texttt{deriv} was assigned the value of 1, placing the intercept of \texttt{dim\_type} in the model right between the means of its two conditions. The motivation for sum-coding is taken from \citeauthor{Winter+2019} (\citeyear{Winter+2019}), who argues that this coding is better suited for mixed-effects models with interactions and random effects; additionally, it is argued to make the model coefficients easier to interpret.

The mixed-effects linear regression model was fitted to the data using the \texttt{lmer()} function from the R package \texttt{lme4} (\cite{lme4+2015}), with \texttt{Anova()} from the package \texttt{car} (\cite{car+2019}) additionally used to extract \textit{p}-values from the model. Packages \texttt{effsize} (\cite{effsize+2020}) and \texttt{MuMIn} (\cite{MuMIn+2022}) were used to extract confidence intervals and $R^2$-values, respectively. Plots were generated using the packages \texttt{ggplot2} (\cite{ggplot2+2016}) and \texttt{sjPlot} (\cite{sjplot+2022}) and their miscellaneous dependencies.

Chapter \ref{chp:results} presents the results.