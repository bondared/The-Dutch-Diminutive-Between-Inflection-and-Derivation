\chapter{Pilot Study Design}\label{chp:study}
The previous chapter saw us adopt the diminutive suffix analysis proposal advanced by \citeauthor{DeBelder+etal+2014} (\citeyear{DeBelder+etal+2014}) and establish predictions for empirical examination of this proposal in a lexical decision experiment. In order to facilitate this examination, a virtual experiment was implemented and carried out on a subset of a pre-existing visual word recognition dataset for Dutch. The entire process of data preparation and analysis was carried out in RStudio (\citeauthor{RStudio}, \citeyear{RStudio}), an open-access development environment for the statistical programming language R (\citeauthor{rlang}, \citeyear{rlang}). All supplementary materials including the experimental dataset, preprocessing and analysis scripts, and digital figures can be found in this OSF repository: \url{https://osf.io/zxjgw/}. 

This chapter presents the practical implementation of the experiment and delves into the methodological and theoretical considerations that went into the decision-making process regarding experimental design. Section~\ref{sec:4-corpus} describes the motivations behind opting for a virtual experiment and choosing the Dutch Lexicon Project 2 (\cite{Brysbaert+etal+2016}) as its base corpus. Section~\ref{sec:4-implementation} then carefully documents the process of introducing the predictors of morphological complexity into the diminutive subcorpus of the DLP2. Finally, Section~\ref{sec:4-analysis} details additional data transformations and outlines the reasons for experimental variable selection. The results yielded by the design as outlined below are subsequently presented in Chapter~\ref{chp:results}.

\section{Data Collection? Corpus Selection!}
\label{sec:4-corpus}
The usual way to test any predictions regarding word recognition is to collect lexical decision data from native speakers of the language(s) in question. However, the process of data collection can be cost-intensive and time-consuming: speakers need to be recruited and vetted based on the aims of the experiment, which is a lengthy process at the best of times. Additionally, participants are usually monetarily compensated for their time, and studies with large groups of people can quickly become very expensive. On the other hand, experiments with very small participant bases have little to no explanatory value, as the sample is simply too small for it to allow accurate predictions about the population at large. Low participant numbers can thus lead to statistically underpowered studies, making the analysis inconclusive at best (\citeauthor{Winter+2019}, \citeyear{Winter+2019}). Software and equipment costs should also be taken into account. The researcher, then, has no choice but to commit significant resources to novel data collection for their experiment, which usually makes it economically unviable for undergraduate students looking to test novel predictions. However, running virtual experiments on previously established corpora such as the Dutch Lexicon Project (DLP2; \cite{Brysbaert+etal+2016}) can provide a convenient alternative.

The Dutch Lexicon Project 2 corpus (DLP2) is a relatively recent addition to the family of lexical decision corpora collected by a group of researchers centered around Marc Brysbaert, who currently leads the Center for Reading Research at the University of Ghent, Belgium. Among others, this collection includes multiple corpora with lexical decision data for English, French, and Dutch. Corpora like the DLP2 typically serve a primary and a secondary goal: to collect new participant data in order to test a prediction, and to provide the resulting experimental dataset to the scientific community so that other researchers can test their hypotheses on a prefabricated dataset before starting off on their own data collection procedures. In fact, some previously established corpora are later used by their creators as auxiliary datasets to introduce new predictors and compare findings between corpora, e.g. \citeauthor{Brysbaert+etal+2016} (\citeyear{Brysbaert+etal+2016}) investigating the effects of word prevalence on the DLP2 and the previously collected Dutch Lexicon Project (\cite{Keuleers+etal+2010b}). 

As the name suggests, the DLP2 is the second and expanded variation of the first DLP, featuring more data points across more participants for more than only mono- and bimorphemic wordforms. Beyond the raw accuracy scores and reaction times, it is ripe with item information on some of the most robust predictors in visual word recognition studies, such as frequency, length, and orthographic similarity. It is freely available and presents observations for 30.000 Dutch lemmas collected from a total of 81 participants. As such, it is very well-suited to serve as the basis of a virtual experiment testing morphological effects on lexical processing in Dutch.

The corpus, however, is not without its limitations. It heavily favours Flemish Dutch, as data collection was primarily carried out in Flanders, with only two speakers originally coming from the Netherlands. The primary concern resulting from this would be the assumption that the \textit{-ke}-forms dominate the diminutive category within the corpus, or are at least at odds with the \textit{-tje}-forms. However, there are only 15 wordforms ending in \textit{-ke}, none of which are diminutive. Nevertheless, effects of dialectal variation could be at play, since the Dutch spoken in the Netherlands and the Dutch spoken in Flanders differ when it comes to choice of vocabulary. In other words, some \textit{-tje}-forms might have elicited lower reaction times from Dutch participants, while others, like the Catholic term \textit{begijntje} "Beguine+DIM", would have probably taken longer to recognize due to their relative obscurity in the Netherlands.

An additional limiting factor is the lack of information regarding each item's morphological makeup, primarily its internal morpheme count. However, given that the underlying number of morphemes assumed for each wordform is heavily theory-specific, the decision to exclude it is understandable. Smaller experimental subsets like the diminutive subcoprus outlined in the following section can skirt around this hindrance by importing the relevant information from other corpora or manual assignment of values to each item.

Despite its minor limitations, the DLP2 was chosen as the underlying dataset for the experimental part of this thesis due to its free availability, impressive amounts of participant data, and the high reliability of reported values for lexical decision predictors. The setup process could now begin in earnest, starting with data cleanup and moving on to the implementation of the experimental design.

\section{Practical Implementation of Predictors}
\label{sec:4-implementation}
Data import and preparation was carried out using R packages \texttt{dplyr} (\citeauthor{dplyr+2022} \citeyear{dplyr+2022}) and \texttt{readr} (\citeauthor{readr+2022}, \citeyear{readr+2022}). The implementation of the virtual experiment proceeded as follows: first, all constituent datasets of the DLP2 corpus were downloaded from \href{http://crr.ugent.be/archives/1796}{its webpage} and imported into Rstudio. The \texttt{items} and \texttt{variables} subcorpora were then merged in order to combine values for lexicality and experimental predictors such as frequency and length in one dataset. Subsequent exclusion of all nonwords from the resulting dataset and filtering it to only include nouns allowed for easier access to the nominal diminutive forms. This subset was then searched for wordforms ending in \textit{-je} to include all possible allomoprhs of the diminutive suffix, resulting in a list of 364 words.

The novel diminutive predictor, named \texttt{dim\_type}, was then set up within the list of diminutives, respectively coding derivational and inflectional forms as \texttt{deriv} and \texttt{infl}. The values for \texttt{dim\_type} were manually assigned, based on the  distinguishing features of the two diminutive types: an item was deemed inflectional if its meaning was compositional (usually reflecting the "small X" pattern) and/or if the process of suffixation had forced a kind-to-item reading shift. Conversely, items with meaning gaps (lexicalised diminutives and \textit{diminutiva tantum}) were counted as derivational. Irregular patterns of allomorphy served as an additional indicator of an item belonging to the derivational category, as per \citeauthor{DeBelder+etal+2009} (\citeyear{DeBelder+etal+2009}). For example, \textit{bloem-pje} "flower+DIM" features the predicted allomorph -\textit{pje} and is treated as inflected, \textit{bloem-etje} has a different allomorph -\textit{pje} and additionally means "bouquet" instead, hence its treatment as derived (\cite{taalportaal}). 

All assigned values were double-checked with the help of native speaker intuitions, as well as additional consultation of online Dutch dictionaries such as the digital version of Van Dale (\url{https://www.vandale.nl/}) and the Dutch version of Wiktionary (\url{https://www.nl.wiktionary.org}), when necessary. Items that had at least one of the inflectional and one of the derivational type readings at the same time were assigned the value \texttt{both} and slated for exclusion from the analysis due to their unpredictability.

An additional variable coding each word for the number of constituent morphemes (\texttt{nmorph}) was set up within the list of diminutives in order to tease apart the straightforward effect of additional morphological complexity and the difference of diminutive type. Due to theoretical assumptions of full decomposition in visual word processing, formal diminutives with no independently occurring base and/or idiosyncratic patterns of meaning were still treated as compositional. Conversely, items ending in \textit{-je} that upon inspection were simplex forms such as \textit{flottielje} "flotilla" and \textit{kastanje} "chestnut" were excluded from the list. Every derivational diminutive was further assigned an extra morpheme to reflect the assumed extra lexical projection. Thus, every item on the list was confirmed to be a diminutive and assigned a value corresponding to its morphological type, and none were assigned an \texttt{nmorph} value lower than 2. The resulting list constituted 356 items in total.

Following the assignment procedure, the diminutive list was then merged with the noun dataset to reintroduce the values for experimental predictors, making up the subcorpus of purely diminutive items. This was subsequently combined with the subcorpora containing raw trial data from the participants recruited for the DLP2 project. An additional cleanup stage filtered out all observations where participants failed to recognise a diminutive as a word, as well as observations for all diminutive items with a mean accuracy lower than 66 per cent. Trials that took longer than 1250 ms were excluded as well. This was intended as an intermediary solution between using 1000ms as the threshold (\cite{Brysbaert+etal+2016} would tell their participants to speed up for the next block if they took longer than 1000ms on average; participants who got three such reminders had all of their responses wiped from the final version of the DLP2) and letting the more extreme values, some going beyond 1500ms, induce an undesirable positive skew in the model used for analysis. Finally, observations for items coded \texttt{both} for \texttt{dim\_type} were excluded from the analysis, resulting in an experimental dataset of 9785 observations across a total of 270 diminutive items.

\section{Preparations for Statistical Analysis}
\label{sec:4-analysis}
The formula for the experimental model was intended to closely follow the one used for reaction times in \citeauthor{Brysbaert+etal+2016} (\citeyear{Brysbaert+etal+2016}); however, due to the differences between the diminutive subset and the entirety of the DLP2, as well as the introduction of \texttt{nmorph} as a predictor, some modifications had to be made. A series of correlation tests between length, number of syllables, \texttt{nmorph}, and orthographic similarity effects revealed a high degree of intercorrelation (average Pearson's $r$=0.7937). Estimating the effects of all four predictors as part of one model would lead to unreliable predictions at best, and so some of them had to be excluded. Due to \texttt{nmorph} being theory-relevant, it was decided to keep it to the detriment of some other predictors. Number of syllables, being the least relevant predictor, was excluded first. Next, length was excluded in favour of orthographic similarity scores, after a series of stepwise model comparisons confirmed that the latter contributed more to explaining the variance in the dataset.

 The observations in the dataset were assumed not to be fully independent for two key reasons.  Firstly, in the design of the DLP2 data collection process, items were repeated across participants. Secondly, the values of \texttt{dim\_type} vary within individuals, as the predictor was not part of the initial design. The data analysis step therefore required using mixed-effects linear regression models to account for possible interdependencies. In order to satisfy the independence assumption for each data point, random intercepts by participant and by item were introduced into the model formula (\cite{Winter+2019}). Introducing an additional random slope by participant resulted in a model with a singular fit, and so the participant slope was removed. To test for possible interaction effects between frequency and diminutive type, an interaction term was introduced as well; a quick comparison using the \texttt{anova()} function confirmed the interaction model to have a better fit to the data. The formula for the complete experimental model is presented in (\ref{ex:modelformula}) in Section~\ref{sec:5-inf_stats}, followed by a discussion of all fixed and random effects.

Reaction times were log-transformed to alleviate the typical positive skew towards longer RTs, as suggested in \citeauthor{Winter+2019} (\citeyear{Winter+2019}). In order to make the difference in-between their effect sizes more comparable, all the continuous predictors were standardised and centered. This process involved subtracting each variable's mean from every data point and then dividing each value of a centered variable by its standard deviation. The values of every continuous predictor were thus linearly transformed into standard units, or \textit{z}-scores, representing how far they were from their respective predictor's mean in terms of standard deviations (\citeauthor{Winter+2019}, \citeyear{Winter+2019}). 

The only categorical predictor, diminutive type, was sum-coded. \texttt{infl} was assigned a contrast value of -1, while \texttt{deriv} was assigned the value of 1, placing the intercept of \texttt{dim\_type} in the model right between the means of its two conditions. The motivation for sum-coding is taken from \citeauthor{Winter+2019} (\citeyear{Winter+2019}), who argues that this coding is better suited for mixed-effects models with interactions and random effects; additionally, it is argued to make the model coefficients easier to interpret.

The mixed-effects linear regression model was fitted to the data using the \texttt{lmer()} function from the R package \texttt{lme4} (\cite{lme4+2015}), with \texttt{Anova()} from the package \texttt{car} (\cite{car+2019}) additionally used to extract \textit{p}-values from the model. Packages \texttt{effsize} (\cite{effsize+2020}) and \texttt{MuMIn} (\cite{MuMIn+2022}) were used to extract confidence intervals and $R^2$-values, respectively. Plots were generated using the packages \texttt{ggplot2} (\cite{ggplot2+2016}) and \texttt{sjPlot} (\cite{sjplot+2022}) and their miscellaneous dependencies. With the technical considerations out of the way, we can now proceed on to the results, which are presented in the following chapter.